
## 量化压缩与模型部署

### 1. 音频流式输入

大多数现有模型只能在整个音频输入信号完整后才开始进行音频编码，大语言模型必须等待完整的视觉/音频输入的编码完成后才能进行处理，从而引入了显著的延迟。为了解决这个问题，我们将音频编码器改造为支持在线流式处理，从而实现更低的延迟。我们将输入音频分割成多个片段，每个片段是表示一秒钟的音频的固定数量音频 token。在音频编码过程中，每个片段都采用因果注意力机制进行编码，仅关注自身及之前的片段，从而满足在线流式编码的需求，同时与离线整体编码相比保持了最小的信息损失。

### 2. 模型量化压缩

量化压缩是端侧部署中常用的一种优化手段，能够通过减少模型大小、提高推理速度、降低功耗、减少带宽需求等方式，提升端侧设备的性能和效率，确保深度学习模型能够在资源有限的设备上高效运行。我们将MiniCPM-o模型进行了4比特量化，并使用量化模型在数据集上进行评估，以衡量因模型量化而带来的损失。同时使用官方开源的pytorch模型在相同数据集下进行实测，并进行评测指标对比。

#### 2.1 量化评估

根据算法模型的应用场景，参考MiniCPM-o官方文档说明[OpenBMB/MiniCPM-o](https://github.com/OpenBMB/MiniCPM-o)，应用StreamingBench数据集[THUNLP-MT/StreamingBench](https://github.com/THUNLP-MT/StreamingBench)对量化、部署后的模型性能在live streaming模式下进行评测。评测数据共计1500条，每条时长不超过1分钟，共包含6类任务，每类任务250条。

数据集的形式为 omni-source understanding (Offline + Text Instruction)，文本指令为QA对的形式，根据时间戳的位置截取视频片段并推理正确答案。文本指令QA对的示例如下：
![qa-pair](../assets/qa-pair.png)

量化部署模型与pytorch相比，在数据集上各个子任务及总体指标情况：

| 模型/方案                    | Misleading Context Understanding | Source Discrimination | Emotion Recognition | Anomaly Context Understanding | Scene Understanding | Multimodal Alignment | 总体精度 | 备注 (各模块精度)            |
| ---------------------------- | -------------------------------- | --------------------- | ------------------- | ----------------------------- | ------------------- | -------------------- | -------- | ---------------------------- |
| pytorch bf16（官方模型）     | 32.8                             | 62.4                  | 48.4                | 38.4                          | 24.4                | 78.4                 | 47.5     | apm(fp16)+vpm(fp16)+llm(f16) |
| pytorch gptq（官方量化模型） | 33.2                             | 54.8                  | 50.8                | 36.4                          | 31.2                | 71.6                 | 46.3     | apm(fp16)+vpm(fp16)+llm(q4)  |
| minicpmo-cpp mix-precision       | 29.2                             | 62.4                  | 44.4                | 41.2                          | 20.8                | 80.8                 | 46.5     | apm(q4)+vpm(fp16)+llm(q4)    |
| minicpmo-cpp full-q4             | 28.0                             | 61.2                  | 48.4                | 37.2                          | 19.6                | 79.2                 | 45.6     | apm(q4)+vpm(q4)+llm(q4)      |

注：
- apm（音频处理模块）: Whisper-300M encoder
- vpm（视觉处理模块）: SigLip-400M
- llm（语言模型）: Qwen2.5-7B

#### 2.2 设备显存占用

在进行设备显存占用的数据测试时，tts（文字转语音）模型处于开发阶段，以下数据均不包含tts模型：

| 模型/方案                    | VRAM(GiB) 推理消耗 | VRAM(GiB) 权重初始化 | 备注 (各模块精度)            |
| ---------------------------- | ------------------ | -------------------- | ---------------------------- |
| pytorch bf16（官方模型）     | 38.04              | 17.22                | apm(fp16)+vpm(fp16)+llm(f16) |
| pytorch gptq（官方量化模型） | 29.23              | 8.51                 | apm(fp16)+vpm(fp16)+llm(q4)  |
| minicpmo-cpp mix-precision       | 7.09               | 6.75                 | apm(q4)+vpm(fp16)+llm(q4)    |
| minicpmo-cpp full-q4             | 6.63               | 6.29                 | apm(q4)+vpm(q4)+llm(q4)      |

注：
- minicpmo-cpp部署时，context size为8192个token，此参数设置越大，VRAM消耗越多，主要为kvcache的空间，此参数下，kvcache buffer为448MiB。

### 3. 推理性能优化

模型经过量化压缩后，可在8GB显存设备上离线运行。模型的目标场景为视频流的处理，因此为了增强用户体验，基于jetson orin nano 8gb设备特性，进行了模型推理的性能优化。最终实现了实时处理和低首token延迟的目标。

#### 3.1 推理速度

英伟达jetson系列设备为统一内存架构设计，内存和显存是物理上共用同一块内存芯片，总内存/带宽资源的共享性带来的潜在竞争问题。为了最大化的优化推理性能，将设备上计算密集的部分常驻在设备内存中，避免程序的内存页被交换（Swap Out）到磁盘上的 Swap 分区或文件，导致推理性能下降。在进行模型推理时，我们将模型的上下文长度设置为4096（可处理30秒视频，推理时内存占用峰值为7GiB左右），同时关闭系统的图形化界面（释放出200MiB内存）。

模型经过优化后，在jetson orin nano super模式下，可实现视频流的实时处理。各模块的推理耗时如下：
| 模块名称                          | 推理耗时 (ms) |
| --------------------------------- | ------------- |
| 视觉编码模块                      | 887.106       |
| 音频编码模块                      | 45.292        |
| 多模态 prefill                    | 40.594        |
| 视觉+音频+模态 prefill+数据预处理 | 990.316       |

#### 3.2 首token延迟

基于音频编码模块流式输入功能的支持与推理速度的优化，我们实现了视频流的实时处理，同时带来了极低的首token延迟。
| 名称           | 耗时            |
| -------------- | --------------- |
| 首token延迟    | 464 ms          |
| decode速度     | 10.39 tokens/s  |

## 三、总结

我们将模型进行4比特量化压缩，在尽量减少量化损失的情况下，大大降低了设备显存的占用。与官方bfloat16精度的pytorch模型相比：
- 在StreamingBench数据集上，针对live streaming任务场景，实测数据在benchmark上掉了2个点（每100个问题，会多2个错误答案）
- 在8192token的上下文（可处理60秒内的视频数据）中，设备显存占用在推理峰值时，由38.04GB降低为7GB左右
